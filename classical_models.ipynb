{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest centroid:\n",
      "predictions: [0 0 1 0 0 2 1 1 1 1 0 0 1 2 1 2 1 1 0 0 2 1 2 0 0 1 2 1 1 1]\n",
      "Actual labels: [0 0 1 0 0 2 2 1 1 1 0 0 1 2 2 2 1 1 0 0 2 2 2 0 0 1 2 1 1 2]\n",
      "score: 0.8667\n",
      "\n",
      "k-NN classifier (k=3):\n",
      "predictions: [0 0 1 0 0 2 1 1 1 1 0 0 1 1 2 2 1 1 0 0 2 2 2 0 0 1 2 1 1 2]\n",
      "Actual labels: [0 0 1 0 0 2 2 1 1 1 0 0 1 2 2 2 1 1 0 0 2 2 2 0 0 1 2 1 1 2]\n",
      "score: 0.9333\n",
      "\n",
      "Naive Bayes classifier (Gaussian):\n",
      "predictions: [0 0 1 0 0 2 1 1 1 1 0 0 1 1 2 2 1 1 0 0 2 2 2 0 0 1 2 1 1 2]\n",
      "Actual labels: [0 0 1 0 0 2 2 1 1 1 0 0 1 2 2 2 1 1 0 0 2 2 2 0 0 1 2 1 1 2]\n",
      "score: 0.9333\n",
      "\n",
      "Naive Bayes classifier (Multinomial):\n",
      "predictions: [0 0 1 0 0 2 2 1 1 1 0 0 1 1 2 2 1 1 0 0 2 2 2 0 0 1 2 1 1 2]\n",
      "Actual labels: [0 0 1 0 0 2 2 1 1 1 0 0 1 2 2 2 1 1 0 0 2 2 2 0 0 1 2 1 1 2]\n",
      "score: 0.9667\n",
      "\n",
      "Decision Tree classifier:\n",
      "predictions: [0 0 1 0 0 2 1 1 1 1 0 0 1 1 1 2 1 1 0 0 2 1 2 0 0 1 2 1 1 2]\n",
      "Actual labels: [0 0 1 0 0 2 2 1 1 1 0 0 1 2 2 2 1 1 0 0 2 2 2 0 0 1 2 1 1 2]\n",
      "score: 0.8667\n",
      "\n",
      "Random Forest classifier (estimators=5):\n",
      "predictions: [1 2 0 1 0 2 2 2 1 0 0 0 0 1 0 2 0 0 1 2 0 1 1 0 2 2 2 2 1 2]\n",
      "Actual labels: [1 2 0 1 0 2 2 2 1 0 0 0 0 1 0 2 0 0 1 2 0 1 1 0 2 1 2 2 1 2]\n",
      "score: 0.9667\n",
      "\n",
      "SVM (linear, C=1.0):\n",
      "predictions: [1 2 0 1 0 2 2 2 1 0 0 0 0 1 0 2 0 0 1 2 0 1 1 0 2 2 2 2 1 2]\n",
      "Actual labels: [1 2 0 1 0 2 2 2 1 0 0 0 0 1 0 2 0 0 1 2 0 1 1 0 2 1 2 2 1 2]\n",
      "score: 0.9667\n",
      "\n",
      "SVM (RBF, C=1.0, gamma=0.25):\n",
      "predictions: [1 2 0 1 0 2 2 2 1 0 0 0 0 1 0 2 0 0 1 2 0 1 1 0 2 2 2 2 1 2]\n",
      "Actual labels: [1 2 0 1 0 2 2 2 1 0 0 0 0 1 0 2 0 0 1 2 0 1 1 0 2 1 2 2 1 2]\n",
      "score: 0.9667\n",
      "\n",
      "SVM (RBF, C=1.0, gamma=0.001, augmented)\n",
      "predictions: [1 2 0 1 0 2 2 2 1 0 0 0 0 1 0 2 0 0 1 2 0 1 1 0 2 2 2 2 1 2]\n",
      "Actual labels: [1 2 0 1 0 2 2 2 1 0 0 0 0 1 0 2 0 0 1 2 0 1 1 0 2 1 2 2 1 2]\n",
      "score: 0.9667\n",
      "\n",
      "SVM (RBF, C=1.0, gamma=0.001, original)\n",
      "predictions: [0 0 1 0 0 2 1 1 1 1 0 0 1 2 1 2 1 1 0 0 2 1 2 0 0 1 2 1 1 1]\n",
      "Actual labels: [0 0 1 0 0 2 2 1 1 1 0 0 1 2 2 2 1 1 0 0 2 2 2 0 0 1 2 1 1 2]\n",
      "score: 0.8667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(f\"predictions: {clf.predict(x_test)}\")\n",
    "    print(f\"Actual labels: {y_test}\")\n",
    "    print(\"score: %0.4f\" % clf.score(x_test, y_test))\n",
    "    print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"./data/iris/iris_features.npy\")\n",
    "    y = np.load(\"./data/iris/iris_labels.npy\")\n",
    "\n",
    "    N = 120\n",
    "    x_train = x[:N]\n",
    "    y_train = y[:N]\n",
    "    x_test = x[N:]\n",
    "    y_test = y[N:]\n",
    "\n",
    "    xa_train=np.load(\"./data/iris/iris_train_features_augmented.npy\")\n",
    "    ya_train=np.load(\"./data/iris/iris_train_labels_augmented.npy\")\n",
    "    xa_test =np.load(\"./data/iris/iris_test_features_augmented.npy\")\n",
    "    ya_test =np.load(\"./data/iris/iris_test_labels_augmented.npy\")\n",
    "\n",
    "    print(\"Nearest centroid:\")\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "    print(\"k-NN classifier (k=3):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "    print(\"Naive Bayes classifier (Gaussian):\")\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "    print(\"Naive Bayes classifier (Multinomial):\")\n",
    "    run(x_train, y_train, x_test, y_test, MultinomialNB())\n",
    "    print(\"Decision Tree classifier:\")\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "    print(\"Random Forest classifier (estimators=5):\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, RandomForestClassifier(n_estimators=5))\n",
    "\n",
    "    print(\"SVM (linear, C=1.0):\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, SVC(kernel=\"linear\", C=1.0))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.25):\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.25))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.001, augmented)\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.001))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.001, original)\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.001))\n",
    "\n",
    "main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest centroid:\n",
      "    score = 0.9386\n",
      "\n",
      "k-NN classifier (k=3):\n",
      "    score = 0.9474\n",
      "\n",
      "k-NN classifier (k=7):\n",
      "    score = 0.9737\n",
      "\n",
      "Naive Bayes classifier (Gaussian):\n",
      "    score = 0.9561\n",
      "\n",
      "Decision Tree classifier:\n",
      "    score = 0.9035\n",
      "\n",
      "Random Forest classifier (estimators=5):\n",
      "    score = 0.9298\n",
      "\n",
      "Random Forest classifier (estimators=50):\n",
      "    score = 0.9561\n",
      "\n",
      "SVM (linear, C=1.0):\n",
      "    score = 0.9825\n",
      "\n",
      "SVM (RBF, C=1.0, gamma=0.03333):\n",
      "    score = 0.9825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"    score = %0.4f\" % clf.score(x_test, y_test))\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"./data/breast_cancer/bc_features_normalized.npy\")\n",
    "    y = np.load(\"./data/breast_cancer/bc_labels.npy\")\n",
    "    N = 455 \n",
    "    x_train = x[:N];  x_test = x[N:]\n",
    "    y_train = y[:N];  y_test = y[N:]\n",
    "\n",
    "    print(\"Nearest centroid:\")\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "    print(\"k-NN classifier (k=3):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "    print(\"k-NN classifier (k=7):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "    print(\"Naive Bayes classifier (Gaussian):\")\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "    print(\"Decision Tree classifier:\")\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "    print(\"Random Forest classifier (estimators=5):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "    print(\"Random Forest classifier (estimators=50):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "    print(\"SVM (linear, C=1.0):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=1.0))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.03333):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.03333))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Running the code multiple times, we observe that, Nearest centroid, K-NN, Naive Bayes, SVM keeps the same score i.e, its constant for the fixed dataset. However, score changes for decision tree and random forest, that is becuse sklearn uses pseudo random sequence to generate random forest and decision tree models. We can add puesdorandom number seed to get the same random sequence to fix this issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest centroid:\n",
      "    score = 0.9386\n",
      "\n",
      "k-NN classifier (k=3):\n",
      "    score = 0.9474\n",
      "\n",
      "k-NN classifier (k=7):\n",
      "    score = 0.9737\n",
      "\n",
      "Naive Bayes classifier (Gaussian):\n",
      "    score = 0.9561\n",
      "\n",
      "Decision Tree classifier:\n",
      "    score = 0.9298\n",
      "\n",
      "Random Forest classifier (estimators=5):\n",
      "    score = 0.9474\n",
      "\n",
      "Random Forest classifier (estimators=50):\n",
      "    score = 0.9561\n",
      "\n",
      "SVM (linear, C=1.0):\n",
      "    score = 0.9825\n",
      "\n",
      "SVM (RBF, C=1.0, gamma=0.03333):\n",
      "    score = 0.9825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"    score = %0.4f\" % clf.score(x_test, y_test))\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"./data/breast_cancer/bc_features_normalized.npy\")\n",
    "    y = np.load(\"./data/breast_cancer/bc_labels.npy\")\n",
    "\n",
    "    # Effects of random split\n",
    "    np.random.seed(12345) # this will fix the ordering pseudo random sequence that sklearn uses to generate tree models --> No change in the output (Decision tree, Random forest)\n",
    "    N = 455 \n",
    "    x_train = x[:N];  x_test = x[N:]\n",
    "    y_train = y[:N];  y_test = y[N:]\n",
    "\n",
    "    print(\"Nearest centroid:\")\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "    print(\"k-NN classifier (k=3):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "    print(\"k-NN classifier (k=7):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "    print(\"Naive Bayes classifier (Gaussian):\")\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "    print(\"Decision Tree classifier:\")\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "    print(\"Random Forest classifier (estimators=5):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "    print(\"Random Forest classifier (estimators=50):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "    print(\"SVM (linear, C=1.0):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=1.0))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.03333):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.03333))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with random seed fixed, score doesn't change(between multiple runs)for any models! great! But also observe that scores for this and previous run is same! i.e same scores for fixed dataset. \n",
    "\n",
    "What happens if we change the dataset ? What if we want to randomly select the datasets ?  \n",
    "\n",
    "idx = np.argsort(np.random.random(y.shape[0]))    \n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "by adding above lines, we will randomly select the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest centroid:\n",
      "    score = 0.9123\n",
      "\n",
      "k-NN classifier (k=3):\n",
      "    score = 0.9825\n",
      "\n",
      "k-NN classifier (k=7):\n",
      "    score = 0.9825\n",
      "\n",
      "Naive Bayes classifier (Gaussian):\n",
      "    score = 0.9035\n",
      "\n",
      "Decision Tree classifier:\n",
      "    score = 0.9298\n",
      "\n",
      "Random Forest classifier (estimators=5):\n",
      "    score = 0.9561\n",
      "\n",
      "Random Forest classifier (estimators=50):\n",
      "    score = 0.9386\n",
      "\n",
      "SVM (linear, C=1.0):\n",
      "    score = 0.9825\n",
      "\n",
      "SVM (RBF, C=1.0, gamma=0.03333):\n",
      "    score = 0.9737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"    score = %0.4f\" % clf.score(x_test, y_test))\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"./data/breast_cancer/bc_features_normalized.npy\")\n",
    "    y = np.load(\"./data/breast_cancer/bc_labels.npy\")\n",
    "\n",
    "    # Effects of random split\n",
    "    np.random.seed(12345) # this will fix the ordering pseudo random sequence that sklearn uses to generate tree models --> No change in the output (Decision tree, Random forest)\n",
    "    idx = np.argsort(np.random.random(y.shape[0]))    \n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "    N = 455 \n",
    "    x_train = x[:N];  x_test = x[N:]\n",
    "    y_train = y[:N];  y_test = y[N:]\n",
    "\n",
    "    print(\"Nearest centroid:\")\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "    print(\"k-NN classifier (k=3):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "    print(\"k-NN classifier (k=7):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "    print(\"Naive Bayes classifier (Gaussian):\")\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "    print(\"Decision Tree classifier:\")\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "    print(\"Random Forest classifier (estimators=5):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "    print(\"Random Forest classifier (estimators=50):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "    print(\"SVM (linear, C=1.0):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=1.0))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.03333):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.03333))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores doesn't change between runs but this is totally different scores compared to first run for example, scores for nearest centroid changed from 0.9386 to 0.9123 score got worse!!!. This is becuase, we changed the dataset. If we change the dataset again, scores may get improved or gets even worse. For some selection of datasets, score improves and for some scores gets worse. K-fold validation helps to remove this dependency on dataset. \n",
    "\n",
    "But, adding random.seed(12345) not only fixed the selection sequence for dataset but also the pseudo random sequence that sklearn uses to generate tree models. by resetting the random.seed(), one can fix sequence the data selection but removes this constraint for the pseudo random sequence that sklearn uses to generate tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"./data/breast_cancer/bc_features_normalized.npy\")\n",
    "y = np.load(\"./data/breast_cancer/bc_labels.npy\")\n",
    "\n",
    "# Controlling random split --> random, but everytime we get the same random idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"    score = %0.4f\" % clf.score(x_test, y_test))\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"./data/breast_cancer/bc_features_normalized.npy\")\n",
    "    y = np.load(\"./data/breast_cancer/bc_labels.npy\")\n",
    "\n",
    "    # Controlling random split --> random, but everytime we get the same random idx\n",
    "    np.random.seed(12345) \n",
    "    idx = np.argsort(np.random.random(y.shape[0]))    \n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    N = 455 \n",
    "    x_train = x[:N];  x_test = x[N:]\n",
    "    y_train = y[:N];  y_test = y[N:]\n",
    "\n",
    "    # resetting the random seed before training and testing so that sklearn can use the psuedo random sequence to generate random tree models \n",
    "    np.random.seed() \n",
    "\n",
    "    print(\"Nearest centroid:\")\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "    print(\"k-NN classifier (k=3):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "    print(\"k-NN classifier (k=7):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "    print(\"Naive Bayes classifier (Gaussian):\")\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "    print(\"Decision Tree classifier:\")\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "    print(\"Random Forest classifier (estimators=5):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "    print(\"Random Forest classifier (estimators=50):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "    print(\"SVM (linear, C=1.0):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=1.0))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.03333):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.03333))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest            : 0.9274 +/- 0.0081 | \n",
      "3-NN               : 0.9664 +/- 0.0063 | \n",
      "7-NN               : 0.9681 +/- 0.0054 | \n",
      "Naive Bayes        : 0.9381 +/- 0.0035 | \n",
      "Decision Tree      : 0.9186 +/- 0.0081 | \n",
      "Random Forest (5)  : 0.9398 +/- 0.0088 | \n",
      "Random Forest (50) : 0.9628 +/- 0.0098 | \n",
      "SVM (linear)       : 0.9664 +/- 0.0068 | \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "def split(x, y, k, m):\n",
    "    ns = int(y.shape[0]/m)\n",
    "    s = []\n",
    "    for i in range(m):\n",
    "        s.append( [ x[(ns*i):(ns*i+ns)],\n",
    "                    y[(ns*i):(ns*i+ns)] ] )\n",
    "    \n",
    "    x_test, y_test = s[k]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(m):\n",
    "        if i == k:\n",
    "            continue\n",
    "        else:\n",
    "            a,b = s[i]\n",
    "            x_train.append(a)\n",
    "            y_train.append(b)\n",
    "    \n",
    "    x_train = np.array(x_train).reshape((m-1)*ns, 30)\n",
    "    y_train = np.array(y_train).reshape((m-1)*ns)\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def pp(z, k, s):\n",
    "    m = z.shape[1]\n",
    "    print(\"%-19s: %0.4f +/- %0.4f | \" %(s, z[k].mean(), z[k].std()/np.sqrt(m)), end='\\n')\n",
    "    # for i in range(m):\n",
    "    #     print(\"%0.4f \" %z[k,i], end='\\n')\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"./data/breast_cancer/bc_features_normalized.npy\")\n",
    "    y = np.load(\"./data/breast_cancer/bc_labels.npy\")\n",
    "    idx = np.argsort(np.random.random(y.shape[0]))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "    m = 5\n",
    "    z = np.zeros((8,m))\n",
    "\n",
    "    for k in range(m):\n",
    "        x_train, y_train, x_test, y_test = split(x,y,k,m)\n",
    "        z[0,k] = run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "        z[1,k] = run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "        z[2,k] = run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "        z[3,k] = run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "        z[4,k] = run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "        z[5,k] = run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "        z[6,k] = run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "        z[7,k] = run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=1.0))\n",
    "\n",
    "    pp(z,0,\"Nearest\")\n",
    "    pp(z,1,\"3-NN\")\n",
    "    pp(z,2,\"7-NN\")  \n",
    "    pp(z,3,\"Naive Bayes\")\n",
    "    pp(z,4,\"Decision Tree\")\n",
    "    pp(z,5,\"Random Forest (5)\")\n",
    "    pp(z,6,\"Random Forest (50)\")\n",
    "    pp(z,7,\"SVM (linear)\")\n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning\n",
    "\n",
    "Lets fine tune k-NN classifier. we will vary $K = [1,3,5,7,9,11,13,15]$ K = 11 seems to the best but you need to run 1000 times and see the results again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-NN               : 0.9540 +/- 0.0046 | \n",
      "3-NN               : 0.9628 +/- 0.0039 | \n",
      "5-NN               : 0.9628 +/- 0.0039 | \n",
      "7-NN               : 0.9664 +/- 0.0039 | \n",
      "9-NN               : 0.9664 +/- 0.0039 | \n",
      "11-NN              : 0.9699 +/- 0.0047 | \n",
      "13-NN              : 0.9664 +/- 0.0030 | \n",
      "15-NN              : 0.9611 +/- 0.0040 | \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "def split(x, y, k, m):\n",
    "    ns = int(y.shape[0]/m)\n",
    "    s = []\n",
    "    for i in range(m):\n",
    "        s.append( [ x[(ns*i):(ns*i+ns)],\n",
    "                    y[(ns*i):(ns*i+ns)] ] )\n",
    "    \n",
    "    x_test, y_test = s[k]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(m):\n",
    "        if i == k:\n",
    "            continue\n",
    "        else:\n",
    "            a,b = s[i]\n",
    "            x_train.append(a)\n",
    "            y_train.append(b)\n",
    "    \n",
    "    x_train = np.array(x_train).reshape((m-1)*ns, 30)\n",
    "    y_train = np.array(y_train).reshape((m-1)*ns)\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def pp(z, k, s):\n",
    "    m = z.shape[1]\n",
    "    print(\"%-19s: %0.4f +/- %0.4f | \" %(s, z[k].mean(), z[k].std()/np.sqrt(m)), end='\\n')\n",
    "    # for i in range(m):\n",
    "    #     print(\"%0.4f \" %z[k,i], end='\\n')\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"./data/breast_cancer/bc_features_normalized.npy\")\n",
    "    y = np.load(\"./data/breast_cancer/bc_labels.npy\")\n",
    "    idx = np.argsort(np.random.random(y.shape[0]))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "    m = 5\n",
    "    z = np.zeros((8,m))\n",
    "\n",
    "    n_hp = [1,3,5,7,9,11,13,15]\n",
    "    for k in range(m):\n",
    "        x_train, y_train, x_test, y_test = split(x,y,k,m)\n",
    "        for n in range(len(n_hp)):\n",
    "            z[n,k] = run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=n_hp[n]))\n",
    "\n",
    "\n",
    "    pp(z,0,\"1-NN\")\n",
    "    pp(z,1,\"3-NN\")\n",
    "    pp(z,2,\"5-NN\")  \n",
    "    pp(z,3,\"7-NN\")\n",
    "    pp(z,4,\"9-NN\")\n",
    "    pp(z,5,\"11-NN\")\n",
    "    pp(z,6,\"13-NN\")\n",
    "    pp(z,7,\"15-NN\")\n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Let's vary the number of trees, $n_t = [5,20,50,100,200,500,1000,5000]$ number for trees = 100 seems to be good. Run it for 1000 times and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (5)  : 0.9504 +/- 0.0178 | \n",
      "Random Forest (20) : 0.9611 +/- 0.0129 | \n",
      "Random Forest (50) : 0.9593 +/- 0.0102 | \n",
      "Random Forest (100): 0.9628 +/- 0.0116 | \n",
      "Random Forest (200)): 0.9558 +/- 0.0115 | \n",
      "Random Forest (500): 0.9611 +/- 0.0114 | \n",
      "Random Forest (1000): 0.9575 +/- 0.0129 | \n",
      "Random Forest (5000): 0.9593 +/- 0.0116 | \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "def split(x, y, k, m):\n",
    "    ns = int(y.shape[0]/m)\n",
    "    s = []\n",
    "    for i in range(m):\n",
    "        s.append( [ x[(ns*i):(ns*i+ns)],\n",
    "                    y[(ns*i):(ns*i+ns)] ] )\n",
    "    \n",
    "    x_test, y_test = s[k]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(m):\n",
    "        if i == k:\n",
    "            continue\n",
    "        else:\n",
    "            a,b = s[i]\n",
    "            x_train.append(a)\n",
    "            y_train.append(b)\n",
    "    \n",
    "    x_train = np.array(x_train).reshape((m-1)*ns, 30)\n",
    "    y_train = np.array(y_train).reshape((m-1)*ns)\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def pp(z, k, s):\n",
    "    m = z.shape[1]\n",
    "    print(\"%-19s: %0.4f +/- %0.4f | \" %(s, z[k].mean(), z[k].std()/np.sqrt(m)), end='\\n')\n",
    "    # for i in range(m):\n",
    "    #     print(\"%0.4f \" %z[k,i], end='\\n')\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"./data/breast_cancer/bc_features_normalized.npy\")\n",
    "    y = np.load(\"./data/breast_cancer/bc_labels.npy\")\n",
    "    idx = np.argsort(np.random.random(y.shape[0]))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "    m = 5\n",
    "    z = np.zeros((8,m))\n",
    "\n",
    "    n_hp = [5,20,50,100,200,500,1000,5000]\n",
    "    for k in range(m):\n",
    "        x_train, y_train, x_test, y_test = split(x,y,k,m)\n",
    "        for n in range(len(n_hp)):\n",
    "            z[n,k] = run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=n_hp[n]))\n",
    "\n",
    "\n",
    "    pp(z,0,\"Random Forest (5)\")\n",
    "    pp(z,1,\"Random Forest (20)\")\n",
    "    pp(z,2,\"Random Forest (50)\")  \n",
    "    pp(z,3,\"Random Forest (100)\")\n",
    "    pp(z,4,\"Random Forest (200))\")\n",
    "    pp(z,5,\"Random Forest (500)\")\n",
    "    pp(z,6,\"Random Forest (1000)\")\n",
    "    pp(z,7,\"Random Forest (5000)\")\n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM's \n",
    "\n",
    "single hyper parameter search, Fudge factor $C = [0.001, 0.01, 0.1, 1.0, 2.0, 10.0, 50.0, 100.0]$ C = 10 seems to be good. Run it 1000 times and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (C = 0.001)    : 0.9381 +/- 0.0043 | \n",
      "SVM (C = 0.01)     : 0.9664 +/- 0.0046 | \n",
      "SVM (C = 0.1)      : 0.9717 +/- 0.0039 | \n",
      "SVM (C = 1)        : 0.9646 +/- 0.0025 | \n",
      "SVM (C = 2)        : 0.9646 +/- 0.0000 | \n",
      "SVM (C = 10)       : 0.9735 +/- 0.0056 | \n",
      "SVM (C = 50)       : 0.9593 +/- 0.0081 | \n",
      "SVM (C = 100)      : 0.9628 +/- 0.0085 | \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "def split(x, y, k, m):\n",
    "    ns = int(y.shape[0]/m)\n",
    "    s = []\n",
    "    for i in range(m):\n",
    "        s.append( [ x[(ns*i):(ns*i+ns)],\n",
    "                    y[(ns*i):(ns*i+ns)] ] )\n",
    "    \n",
    "    x_test, y_test = s[k]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(m):\n",
    "        if i == k:\n",
    "            continue\n",
    "        else:\n",
    "            a,b = s[i]\n",
    "            x_train.append(a)\n",
    "            y_train.append(b)\n",
    "    \n",
    "    x_train = np.array(x_train).reshape((m-1)*ns, 30)\n",
    "    y_train = np.array(y_train).reshape((m-1)*ns)\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def pp(z, k, s):\n",
    "    m = z.shape[1]\n",
    "    print(\"%-19s: %0.4f +/- %0.4f | \" %(s, z[k].mean(), z[k].std()/np.sqrt(m)), end='\\n')\n",
    "    # for i in range(m):\n",
    "    #     print(\"%0.4f \" %z[k,i], end='\\n')\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"./data/breast_cancer/bc_features_normalized.npy\")\n",
    "    y = np.load(\"./data/breast_cancer/bc_labels.npy\")\n",
    "    idx = np.argsort(np.random.random(y.shape[0]))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "    m = 5\n",
    "    z = np.zeros((8,m))\n",
    "\n",
    "    n_hp = [0.001, 0.01, 0.1, 1.0, 2.0, 10.0, 50.0, 100.0]\n",
    "    for k in range(m):\n",
    "        x_train, y_train, x_test, y_test = split(x,y,k,m)\n",
    "        for n in range(len(n_hp)):\n",
    "            z[n,k] = run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=n_hp[n]))\n",
    "\n",
    "\n",
    "    pp(z,0,\"SVM (C = 0.001)\")\n",
    "    pp(z,1,\"SVM (C = 0.01)\")\n",
    "    pp(z,2,\"SVM (C = 0.1)\")  \n",
    "    pp(z,3,\"SVM (C = 1)\")\n",
    "    pp(z,4,\"SVM (C = 2)\")\n",
    "    pp(z,5,\"SVM (C = 10)\")\n",
    "    pp(z,6,\"SVM (C = 50)\")\n",
    "    pp(z,7,\"SVM (C = 100)\")\n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM, dual parameter search, Fudge factor $C = [0.001, 0.01, 0.1, 1.0, 2.0, 10.0, 50.0, 100.0]$ and $RBF = (1./30)*2.0**np.array([-4,-3,-2,-1,0,1,2,3])$ as a grid search (2D array of C & RBF). We can also run an optimizer to optimize both values as these are continuous values not discrete values. C = 10 seems to be good. Run it 1000 times and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C     = 10.00000\n",
      "     gamma = 0.00208\n",
      "   accuracy= 0.98053\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "def split(x,y,k,m):\n",
    "    ns = int(y.shape[0]/m)\n",
    "    s = []\n",
    "    for i in range(m):\n",
    "        s.append([x[(ns*i):(ns*i+ns)], y[(ns*i):(ns*i+ns)]])\n",
    "    x_test, y_test = s[k]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(m):\n",
    "        if (i==k):\n",
    "            continue\n",
    "        else:\n",
    "            a,b = s[i]\n",
    "            x_train.append(a)\n",
    "            y_train.append(b)\n",
    "    x_train = np.array(x_train).reshape(((m-1)*ns,30))\n",
    "    y_train = np.array(y_train).reshape((m-1)*ns)\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def main():\n",
    "    m = 5 \n",
    "    x = np.load(\"./data/breast_cancer/bc_features_normalized.npy\")\n",
    "    y = np.load(\"./data/breast_cancer/bc_labels.npy\")\n",
    "    idx = np.argsort(np.random.random(y.shape[0]))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    Cs = np.array([0.01,0.1,1.0,2.0,10.0,50.0,100.0])\n",
    "    gs = (1./30)*2.0**np.array([-4,-3,-2,-1,0,1,2,3])\n",
    "    zmax = 0.0 \n",
    "    for C in Cs: \n",
    "        for g in gs: \n",
    "            z = np.zeros(m)\n",
    "            for k in range(m):\n",
    "                x_train, y_train, x_test, y_test = split(x,y,k,m)\n",
    "                z[k] = run(x_train, y_train, x_test, y_test, SVC(C=C,gamma=g,kernel=\"rbf\"))\n",
    "            if (z.mean() > zmax):\n",
    "                zmax = z.mean()\n",
    "                bestC = C \n",
    "                bestg = g \n",
    "    print(\"best C     = %0.5f\" % bestC)\n",
    "    print(\"     gamma = %0.5f\" % bestg)\n",
    "    print(\"   accuracy= %0.5f\" % zmax)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
